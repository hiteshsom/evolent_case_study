{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7d9bbf86da5e"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99c1c3fc2ca5"
   },
   "source": [
    "# Vertex AI Model Garden - Falcon Instruct (PEFT)\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_falcon_instruct_peft.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_falcon_instruct_peft.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_pytorch_falcon_instruct_peft.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "Open in Vertex AI Workbench\n",
    "    </a> (A Python-3 GPU notebook is recommended)\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3de7470326a2"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates running inferences locally with prebuilt Falcon Instruct models, deploying prebuilt Falcon Instruct models, finetuning and deploying Falcon Instruct models with performance efficient finetuning libraries ([PEFT](https://github.com/huggingface/peft)), quantizing and deploying Falcon Instruct models with [GPTQ](https://arxiv.org/abs/2210.17323), and evaluating PEFT-finetuned Falcon Instruct models in Vertex AI.\n",
    "\n",
    "### Objective\n",
    "\n",
    "- Run inferences locally on prebuilt Falcon Instruct models\n",
    "- Deploy prebuilt Falcon Instruct models\n",
    "- Finetune and deploy Falcon Instruct models with PEFT\n",
    "- Quantize and deploy Falcon Instruct models with GPTQ\n",
    "- Evaluate PEFT-finetuned Falcon Instruct models\n",
    "\n",
    "| Models | LoRA |\n",
    "| :- | :- |\n",
    "| [tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) | Y |\n",
    "| [tiiuae/falcon-40b-instruct](https://huggingface.co/tiiuae/falcon-40b-instruct) | Y |\n",
    "\n",
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "264c07757582"
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "**NOTE**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands.\n",
    "\n",
    "Running inferences locally with Falcon Instruct models requires a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioensNKM8ned"
   },
   "source": [
    "### Colab only\n",
    "Run the following commands for Colab and skip this section if you are using Workbench."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 108297,
     "status": "ok",
     "timestamp": 1707152390991,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "2707b02ef5df",
    "outputId": "69c9bbc1-7943-4a38-fa12-b5b6ab9e9551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.10/dist-packages (1.39.0)\n",
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.40.0-py2.py3-none-any.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.11.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.23.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.20.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (23.2)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.8.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.12.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.11.0)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.0.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.62.0)\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.17.3)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.51.3)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.48.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.0)\n",
      "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.10/dist-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.23.5)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.3.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2023.11.17)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.5.1)\n",
      "Installing collected packages: google-cloud-aiplatform\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.39.0\n",
      "    Uninstalling google-cloud-aiplatform-1.39.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.39.0\n",
      "Successfully installed google-cloud-aiplatform-1.40.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "google"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-language==2.10.0\n",
      "  Downloading google_cloud_language-2.10.0-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.0/101.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-language==2.10.0) (2.11.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-language==2.10.0) (1.23.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-language==2.10.0) (3.20.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language==2.10.0) (1.62.0)\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language==2.10.0) (2.17.3)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language==2.10.0) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language==2.10.0) (1.51.3)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language==2.10.0) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language==2.10.0) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language==2.10.0) (0.3.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language==2.10.0) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language==2.10.0) (4.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language==2.10.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language==2.10.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language==2.10.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language==2.10.0) (2023.11.17)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-language==2.10.0) (0.5.1)\n",
      "Installing collected packages: google-cloud-language\n",
      "  Attempting uninstall: google-cloud-language\n",
      "    Found existing installation: google-cloud-language 2.9.1\n",
      "    Uninstalling google-cloud-language-2.9.1:\n",
      "      Successfully uninstalled google-cloud-language-2.9.1\n",
      "Successfully installed google-cloud-language-2.10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "google"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.31.0\n",
      "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2023.11.17)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.0\n",
      "    Uninstalling tokenizers-0.15.0:\n",
      "      Successfully uninstalled tokenizers-0.15.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.35.2\n",
      "    Uninstalling transformers-4.35.2:\n",
      "      Successfully uninstalled transformers-4.35.2\n",
      "Successfully installed tokenizers-0.13.3 transformers-4.31.0\n",
      "Collecting einops==0.6.1\n",
      "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m707.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: einops\n",
      "Successfully installed einops-0.6.1\n",
      "Collecting accelerate==0.21.0\n",
      "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (2.1.0+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.21.0) (1.3.0)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.21.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    ! pip3 install --upgrade google-cloud-aiplatform\n",
    "    ! pip3 install google-cloud-language==2.10.0\n",
    "    from google.colab import auth as google_auth\n",
    "\n",
    "    google_auth.authenticate_user()\n",
    "\n",
    "    # Restart the notebook kernel after installs.\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n",
    "! pip3 install transformers==4.31.0\n",
    "! pip3 install einops==0.6.1\n",
    "! pip3 install accelerate==0.21.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb7adab99e41"
   },
   "source": [
    "### Setup Google Cloud project\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Vertex AI API, Compute Engine API, and Cloud Natural Language API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component,language.googleapis.com).\n",
    "\n",
    "1. [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs.\n",
    "\n",
    "1. [Create a service account](https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console) with `Vertex AI User` and `Storage Object Admin` roles for deploying fine tuned model to Vertex AI endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6c460088b873"
   },
   "source": [
    "Set the following variables for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the specified region (`REGION`). Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5645,
     "status": "ok",
     "timestamp": 1707157265978,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "855d6b96f291",
    "outputId": "bfb9e0ad-1beb-41f1-f4c3-e34628f88b41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "# Cloud project id.\n",
    "PROJECT_ID = \"<your project id>\"  # @param {type:\"string\"}\n",
    "\n",
    "# The region you want to launch jobs in.\n",
    "REGION = \"<region>\"  # @param {type:\"string\"}\n",
    "\n",
    "# The Cloud Storage bucket for storing experiments output with gs:// prefix.\n",
    "BUCKET_URI = \"<your cloud storage bucket url>\"  # @param {type:\"string\"}\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "\n",
    "import os\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "EXPERIMENT_BUCKET = os.path.join(BUCKET_URI, \"peft\")\n",
    "DATA_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"data\")\n",
    "MODEL_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"model\")\n",
    "\n",
    "# The service account looks like:\n",
    "# '@.iam.gserviceaccount.com'\n",
    "# Please go to https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console\n",
    "# and create service account with `Vertex AI User` and `Storage Object Admin` roles.\n",
    "# The service account for deploying fine tuned model.\n",
    "SERVICE_ACCOUNT = \"<your service account>\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e828eb320337"
   },
   "source": [
    "### Initialize Vertex AI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12cd25839741"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cc825514deb"
   },
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b42bd4fa2b2d"
   },
   "outputs": [],
   "source": [
    "# The pre-built training, serving and evaluation docker images.\n",
    "TRAIN_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train:20231207_0936_RC00\"\n",
    "PREDICTION_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-serve:20231129_0948_RC00\"\n",
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20231127_0916_RC00\"\n",
    "VLLM_GPTQ_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:gptq\"\n",
    "EVAL_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-lm-evaluation-harness:20231011_0934_RC00\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c250872074f"
   },
   "source": [
    "### Define common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "354da31189dc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Tuple\n",
    "\n",
    "from google.cloud import aiplatform, language\n",
    "\n",
    "\n",
    "def get_job_name_with_datetime(prefix: str) -> str:\n",
    "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
    "    jobs in Vertex AI.\n",
    "    \"\"\"\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "def deploy_model(\n",
    "    model_name: str,\n",
    "    base_model_id: str,\n",
    "    finetuned_lora_model_path: str,\n",
    "    service_account: str,\n",
    "    task: str,\n",
    "    machine_type: str = \"n1-standard-16\",\n",
    "    # accelerator_type: str = \"NVIDIA_TESLA_V100\",\n",
    "    accelerator_type: str = \"NVIDIA_TESLA_P100\",\n",
    "    accelerator_count: int = 1,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys trained models into Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "    serving_env = {\n",
    "        \"BASE_MODEL_ID\": base_model_id,\n",
    "        \"TASK\": task,\n",
    "    }\n",
    "    if finetuned_lora_model_path:\n",
    "        serving_env[\"FINETUNED_LORA_MODEL_PATH\"] = finetuned_lora_model_path\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=PREDICTION_DOCKER_URI,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/predictions/peft_serving\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=serving_env,\n",
    "    )\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "    return model, endpoint\n",
    "\n",
    "\n",
    "def deploy_model_vllm(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"n1-standard-16\",\n",
    "    # accelerator_type: str = \"NVIDIA_TESLA_V100\",\n",
    "    accelerator_type: str = \"NVIDIA_TESLA_P100\",\n",
    "    accelerator_count: int = 1,\n",
    "    quantization_method: str = \"\",\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "\n",
    "    vllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        \"--gpu-memory-utilization=0.9\",\n",
    "        \"--disable-log-stats\",\n",
    "        \"--dtype=float16\",\n",
    "        \"--trust-remote-code\",\n",
    "    ]\n",
    "    if quantization_method:\n",
    "        vllm_args.append(f\"--quantization={quantization_method}\")\n",
    "    if quantization_method == \"gptq\":\n",
    "        vllm_docker_uri = VLLM_GPTQ_DOCKER_URI\n",
    "    else:\n",
    "        vllm_docker_uri = VLLM_DOCKER_URI\n",
    "\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=vllm_docker_uri,\n",
    "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "    return model, endpoint\n",
    "\n",
    "\n",
    "def moderate_text(text: str) -> language.ModerateTextResponse:\n",
    "    \"\"\"Calls Vertex AI APIs to analyze text moderations.\"\"\"\n",
    "    client = language.LanguageServiceClient()\n",
    "    document = language.Document(\n",
    "        content=text,\n",
    "        type_=language.Document.Type.PLAIN_TEXT,\n",
    "    )\n",
    "    return client.moderate_text(document=document)\n",
    "\n",
    "\n",
    "def show_text_moderation(text: str, response: language.ModerateTextResponse) -> None:\n",
    "    \"\"\"Shows text moderation results.\"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    def confidence(category: language.ClassificationCategory) -> float:\n",
    "        return category.confidence\n",
    "\n",
    "    columns = [\"category\", \"confidence\"]\n",
    "    categories = sorted(response.moderation_categories, key=confidence, reverse=True)\n",
    "    data = ((category.name, category.confidence) for category in categories)\n",
    "    df = pd.DataFrame(columns=columns, data=data)\n",
    "\n",
    "    print(f\"Text analyzed:\\n{text}\")\n",
    "    print(df.to_markdown(index=False, tablefmt=\"presto\", floatfmt=\".0%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06e73cb3f412"
   },
   "source": [
    "## Run inferences locally with prebuilt Falcon Instruct models\n",
    "\n",
    "You will need at least 16GB of memory to swiftly run inference with Falcon-7B-Instruct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225,
     "referenced_widgets": [
      "177a2ca248a04f018f2b33c62a0b69ad",
      "84823bb80b4e46059ec9475365eb50f7",
      "a7d695824ab24305886e9a99e9c18ee4",
      "47c36d1369ff4bf69cf2f08d417ec44d",
      "43bb13013339485ea1db7307f574f0dd",
      "6aa23af6554f41cba9744250bd0eabc2",
      "e5c32638f4244f1fb3c005e48a10911e",
      "4dc1bce972934f639a73d749267d2847",
      "9396e90f6c2247d8bfb018617241ef43",
      "1c0b3e3c081745b6b468f320c9fb1e92",
      "b00f3bad6d704cf8a91b1c49053299d4"
     ]
    },
    "id": "3ea64305957f",
    "outputId": "6578d45f-588a-4462-c192-ced3aec234b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "WARNING:transformers_modules.tiiuae.falcon-7b-instruct.cf4b3c42ce2fdfe24f753f0f0d179202fea59c99.configuration_falcon:\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177a2ca248a04f018f2b33c62a0b69ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "sequences = pipeline(\n",
    "    \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Girafatron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8neJc8CnDDpu"
   },
   "source": [
    "## Deploy prebuilt Falcon Instruct models\n",
    "\n",
    "This section deploys prebuilt Falcon Instruct models on the Endpoint. The model deployment step will take 15 minutes to 40 minutes to complete.\n",
    "\n",
    "The peak GPU memory usages for [tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct), and [tiiuae/falcon-40b-instruct](https://huggingface.co/tiiuae/falcon-40b-instruct) are ~15.5G and ~84G separately with the default settings. Please adjust the machine type, accelerator type and accelerator count accordingly. We use V100 in deployments as an example. Note that V100 serving generally offers better throughput and latency performance than L4 serving, while L4 serving is generally more cost efficient than V100 serving. The serving efficiency of V100 and L4 GPUs is inferior to that of A100 GPUs, but V100 and L4 GPUs are nevertheless good serving solutions if you do not have A100 quota."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MjaORIIFDVu"
   },
   "source": [
    "Set the prebuilt model id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8OiHHNNE_wj"
   },
   "outputs": [],
   "source": [
    "prebuilt_model_id = \"tiiuae/falcon-7b-instruct\"  # @param [\"tiiuae/falcon-7b-instruct\", \"tiiuae/falcon-40b-instruct\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHFW7yvjaVFV"
   },
   "source": [
    "We use the PEFT serving images to deploy prebuilt Falcon Instruct models, by setting finetuning LoRA model paths as empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uak1pyEeExYM"
   },
   "outputs": [],
   "source": [
    "# Find Vertex AI supported accelerators and regions in:\n",
    "#  https://cloud.google.com/vertex-ai/docs/predictions/configure-compute\n",
    "\n",
    "# Sets V100 (16G) to deploy tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
    "# If A100 is not available, you may deploy tiiuae/falcon-40b-instruct with\n",
    "#  multiple V100s. Please keep in mind that the efficiency of serving with\n",
    "#  multiple V100s is inferior to that of serving with A100s.\n",
    "# Compared with L4, V100 serving can have better throughput and latency.\n",
    "machine_type = \"n1-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "accelerator_type = \"NVIDIA_TESLA_P100\"\n",
    "accelerator_count = 1  # for tiiuae/falcon-7b-instruct\n",
    "# accelerator_count = 8  # for tiiuae/falcon-40b-instruct\n",
    "\n",
    "# Sets L4 (24G) to deploy tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
    "# If A100 is not available, you may deploy tiiuae/falcon-40b-instruct with\n",
    "#  multiple L4s. Please keep in mind that the efficiency of serving with\n",
    "#  multiple L4s is inferior to that of serving with A100s.\n",
    "# Compared with V100, L4 serving can be more cost efficient.\n",
    "\n",
    "# For tiiuae/falcon-7b-instruct.\n",
    "# machine_type = \"g2-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 1\n",
    "\n",
    "# For tiiuae/falcon-40b-instruct.\n",
    "# machine_type = \"g2-standard-48\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 4\n",
    "\n",
    "# Sets A100 (40G) to deploy tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
    "# machine_type = \"a2-highgpu-1g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "# accelerator_count = 1  # for tiiuae/falcon-7b-instruct\n",
    "# accelerator_count = 4  # for tiiuae/falcon-40b-instruct\n",
    "\n",
    "# Sets A100 (80G) to deploy falcon-40b-instruct models for faster inferences.\n",
    "# machine_type = \"a2-ultragpu-1g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "# accelerator_count = 2\n",
    "\n",
    "model_without_peft, endpoint_without_peft = deploy_model(\n",
    "    model_name=get_job_name_with_datetime(prefix=\"falcon-instruct-serve\"),\n",
    "    base_model_id=prebuilt_model_id,\n",
    "    finetuned_lora_model_path=\"\",  # This will avoid override finetuning models.\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    task=\"instruct-lora\",\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    ")\n",
    "print(\"endpoint_name:\", endpoint_without_peft.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGKIjgmDFRW2"
   },
   "source": [
    "NOTE: The prebuilt model weights will be downloaded on the fly from the original location after the deployment succeeds. Thus, an additional 10-30 minutes of waiting time is needed **after** the above model deployment step succeeds and before you can run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint.\n",
    "\n",
    "Once deployment succeeds, you can send requests to the endpoint with text prompts.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Human: What is a car?\n",
    "Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2251,
     "status": "ok",
     "timestamp": 1707069281448,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "rDHsCOqvFYBi",
    "outputId": "cba7634c-7bf8-4a5e-b5a0-8a9c60efb6e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "What is the capital of India\n",
      "Output:\n",
      "What is the capital of India?\n",
      "The capital of India is New Delhi, which is located in the northern part of the country.\n"
     ]
    }
   ],
   "source": [
    "# Loads an existing endpoint instance using the endpoint name:\n",
    "# - Using `endpoint_name = endpoint_without_peft.name` allows us to get the\n",
    "#   endpoint name of the endpoint `endpoint_without_peft` created in the cell\n",
    "#   above.\n",
    "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
    "#   an existing endpoint with the ID 1234567890123456789.\n",
    "# You may uncomment the code below to load an existing endpoint.\n",
    "\n",
    "# endpoint_name = endpoint_without_peft.name\n",
    "endpoint_name = \"<endpoint id>\"  # @param {type:\"string\"}\n",
    "aip_endpoint_name = (\n",
    "    f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    ")\n",
    "endpoint_without_peft = aiplatform.Endpoint(aip_endpoint_name)\n",
    "\n",
    "instances = [\n",
    "    # {\n",
    "    #     \"prompt\": \"What is a car?\",\n",
    "    #     \"max_tokens\": 50,\n",
    "    #     \"temperature\": 1.0,\n",
    "    #     \"top_p\": 1.0,\n",
    "    #     \"top_k\": 10,\n",
    "    # },\n",
    "    {\n",
    "        \"prompt\": \"What is the capital of India\",\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 10,\n",
    "    },\n",
    "]\n",
    "response = endpoint_without_peft.predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 807,
     "status": "ok",
     "timestamp": 1707069293285,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "ugng2a67LcfI",
    "outputId": "db791e12-b902-4dc3-a0c9-f98d7dec2b10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=['Prompt:\\nWhat is the capital of India\\nOutput:\\nWhat is the capital of India?\\nThe capital of India is New Delhi, which is located in the northern part of the country.'], deployed_model_id='574549801094348800', model_version_id='1', model_resource_name='projects/698487201632/locations/us-central1/models/1700915900867149824', explanations=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e70e3519ff8b"
   },
   "source": [
    "## Finetune and deploy Falcon Instruct models with PEFT\n",
    "\n",
    "This section demonstrates how to finetune and deploy Falcon Instruct models with PEFT LoRA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qCrm_kJH5cz"
   },
   "source": [
    "Set the base model id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3UBLiYrM3sU"
   },
   "outputs": [],
   "source": [
    "base_model_id = \"tiiuae/falcon-7b-instruct\"  # @param [\"tiiuae/falcon-7b-instruct\", \"tiiuae/falcon-40b-instruct\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWGwJHqI7LMs"
   },
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKEYoRfiHDVv"
   },
   "source": [
    "Use the Vertex AI SDK to create and run the custom training jobs with Vertex AI Model Garden training images.\n",
    "\n",
    "This example uses the dataset [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco). You can either use a [dataset from huggingface](https://huggingface.co/datasets) or a custom JSONL dataset in [Vertex text model dataset format](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-supervised#dataset-format) stored in Cloud Storage. The `template` parameter is optional.\n",
    "\n",
    "The peak GPU memory usages are ~11G and ~34G for finetuning LoRA models for [tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct), and [tiiuae/falcon-40b-instruct](https://huggingface.co/tiiuae/falcon-40b-instruct) separately with default training parameters and the example dataset. Falcon-7b-instruct can be finetuned on 1 P100/V100, and falcon-40b-instruct can be finetuned on 1 A100 (40G)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d272b9f416a9"
   },
   "source": [
    "#### [Optional] Finetune with a custom dataset\n",
    "\n",
    "To use a custom dataset, you should supply a `gs://` URI to a JSONL file in [Vertex text model dataset format](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-supervised#dataset-format) in the `dataset_name` below.\n",
    "\n",
    "For example, here is one data point from the sample dataset `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`:\n",
    "\n",
    "```json\n",
    "{\"input_text\":\"TRANSCRIPT: \\nREASON FOR EVALUATION:,\\n\\n LABEL:\",\"output_text\":\"Chiropractic\"}\n",
    "```\n",
    "\n",
    "To use this sample dataset that contains `input_text` and `output_text` fields, set `dataset_name` to `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl` and `template` to `vertex_sample`. For advanced usage with custom datatset fields, see [the template example](https://github.com/tloen/alpaca-lora/blob/main/templates/alpaca.json) and supply your own JSON template as `gs://` URIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "executionInfo": {
     "elapsed": 1546,
     "status": "error",
     "timestamp": 1707145784877,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "584fB6dXpWMz",
    "outputId": "58a204c7-d9fc-44b6-e8f9-628e94bcef4f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BUCKET_URI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1f6d15456c8d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mBUCKET_URI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'BUCKET_URI' is not defined"
     ]
    }
   ],
   "source": [
    "BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 824,
     "status": "error",
     "timestamp": 1707315673146,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "pTFszf1lnip7",
    "outputId": "22f6f171-94a8-42d2-cb03-c2c8bae732d4"
   },
   "outputs": [
    {
     "ename": "NotFound",
     "evalue": "404 GET https://storage.googleapis.com/storage/v1/b/evolent_data_hitesh?projection=noAcl&prettyPrint=false: The specified bucket does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-69d32299fe4d>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstorage_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mbucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_bucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'evolent_data_hitesh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# blob = bucket.blob('train.dat')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/storage/client.py\u001b[0m in \u001b[0;36mget_bucket\u001b[0;34m(self, bucket_or_name, timeout, if_metageneration_match, if_metageneration_not_match, retry)\u001b[0m\n\u001b[1;32m    770\u001b[0m         \"\"\"\n\u001b[1;32m    771\u001b[0m         \u001b[0mbucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bucket_arg_to_bucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_or_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m         bucket.reload(\n\u001b[0m\u001b[1;32m    773\u001b[0m             \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/storage/bucket.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self, client, projection, timeout, if_etag_match, if_etag_not_match, if_metageneration_match, if_metageneration_not_match, retry)\u001b[0m\n\u001b[1;32m   1084\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mHow\u001b[0m \u001b[0mto\u001b[0m \u001b[0mretry\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mRPC\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mSee\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mconfiguring_retries\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \"\"\"\n\u001b[0;32m-> 1086\u001b[0;31m         super(Bucket, self).reload(\n\u001b[0m\u001b[1;32m   1087\u001b[0m             \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0mprojection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/storage/_helpers.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self, client, projection, if_etag_match, if_etag_not_match, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, retry)\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_etag_match\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mif_etag_match\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_etag_not_match\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mif_etag_not_match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         )\n\u001b[0;32m--> 246\u001b[0;31m         api_response = client._get_resource(\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mquery_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/storage/client.py\u001b[0m in \u001b[0;36m_get_resource\u001b[0;34m(self, path, query_params, headers, timeout, retry, _target_object)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbucket\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[0;32m--> 377\u001b[0;31m         return self._connection.api_request(\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/storage/_http.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             )\n\u001b[0;32m--> 349\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    350\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msleep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msleep_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/_http/__init__.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexpect_json\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFound\u001b[0m: 404 GET https://storage.googleapis.com/storage/v1/b/evolent_data_hitesh?projection=noAcl&prettyPrint=false: The specified bucket does not exist."
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "# BUCKET_NAME = \"gs://my_bucket\"\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket('<your cloud storage bucket>')\n",
    "\n",
    "# blob = bucket.blob('train.dat')\n",
    "# blob = blob.download_as_string()\n",
    "# blob = blob.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TRI2_Svo446"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# {\n",
    "#     \"description\": \"Template used by Alpaca-LoRA.\",\n",
    "#     \"prompt_input\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n",
    "#     \"prompt_no_input\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\",\n",
    "#     \"response_split\": \"### Response:\"\n",
    "# }\n",
    "\n",
    "def preprocess_data(bucket, dataset):\n",
    "  blob = bucket.blob(f\"{dataset}.dat\")\n",
    "  blob = blob.download_as_string()\n",
    "  blob = blob.decode('utf-8')\n",
    "  data_examples = blob.split('\\n')\n",
    "  tab_char = '\\t'\n",
    "  print(type(data_examples))\n",
    "  label_map = {'1': 'digestive system diseases',\n",
    "               '2': 'cardiovascular diseases',\n",
    "               '3': 'neoplasms',\n",
    "               '4': 'nervous system diseases',\n",
    "               '5': 'general pathological conditions'}\n",
    "  final_data = []\n",
    "  try:\n",
    "    for example in data_examples:\n",
    "      final_data.append(str({'input_text': f\"You are a medical conversational bot. Classify the given text into an appropriate disease group. The groups are: [digestive system diseases, cardiovascular diseases, neoplasms, nervous system diseases, general pathological conditions] Text: {example.split(tab_char)[1]}\", 'output_text': f\"{label_map[example.split(tab_char)[0]]}\"}))\n",
    "  except Exception as e:\n",
    "    pass\n",
    "  print(len(final_data))\n",
    "  print(final_data[0])\n",
    "\n",
    "  # instruction = 'Describe the type of disease'\n",
    "  # ex_desc = {\n",
    "  #               \"description\": \"Template used by Alpaca-LoRA.\",\n",
    "  #               \"prompt_input\": f\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n",
    "  #               \"prompt_no_input\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\",\n",
    "  #               \"response_split\": \"### Response:\"\n",
    "  #           }\n",
    "  data_jsonl = \"\\n\".join(final_data)\n",
    "\n",
    "  # with open(f'{dataset}.jsonl', 'w') as f:\n",
    "  #   f.write(data_jsonl)\n",
    "\n",
    "  # create a blob\n",
    "  blob = bucket.blob(f'{dataset}.jsonl')\n",
    "  # upload the blob\n",
    "  blob.upload_from_string(\n",
    "        data=json.dumps(data_jsonl)\n",
    "        )\n",
    "\n",
    "  # return data_examples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 614,
     "status": "ok",
     "timestamp": 1707049095707,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "7mxBy02gyKCb",
    "outputId": "aa39133b-7c83-45ac-ebef-db6ed0161da3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "14438\n",
      "{'input_text': 'You are a medical conversational bot. Classify the given text into an appropriate disease group. The groups are: [digestive system diseases, cardiovascular diseases, neoplasms, nervous system diseases, general pathological conditions] Text: Catheterization laboratory events and hospital outcome with direct angioplasty for acute myocardial infarction To assess the safety of direct infarct angioplasty without antecedent thrombolytic therapy, catheterization laboratory and hospital events were assessed in consecutively treated patients with infarctions involving the left anterior descending (n = 100 patients), right (n = 100), and circumflex (n = 50) coronary arteries. The groups of patients were similar for age (left anterior descending coronary artery, 59 years; right coronary artery, 58 years; circumflex coronary artery, 62 years), patients with multivessel disease (left anterior descending coronary artery, 55%; right coronary artery, 55%; circumflex coronary artery, 64%), and patients with initial grade 0/1 antegrade flow (left anterior descending coronary artery, 79%; right coronary artery, 84%; circumflex coronary artery, 90%). Cardiogenic shock was present in eight patients with infarction of the left anterior descending coronary artery, four with infarction of the right coronary artery, and four with infarction of the circumflex coronary artery. Major catheterization laboratory events (cardioversion, cardiopulmonary resuscitation, dopamine or intra-aortic balloon pump support for hypotension, and urgent surgery) occurred in 10 patients with infarction of the left anterior descending coronary artery, eight with infarction of the right coronary artery, and four with infarction of the circumflex coronary artery (16 of 16 shock and six of 234 nonshock patients, p less than 0.001). There was one in-laboratory death (shock patient with infarction of the left anterior descending coronary artery). ', 'output_text': 'nervous system diseases'}\n"
     ]
    }
   ],
   "source": [
    "preprocess_data(bucket, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "executionInfo": {
     "elapsed": 629,
     "status": "error",
     "timestamp": 1707050925884,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "65467b361315",
    "outputId": "3d4f3e05-d09e-4575-974e-18c59372e5af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n",
      "gs://evolent_data_hitesh/temporal/aiplatform-custom-training-2024-02-04-12:48:44.568 \n"
     ]
    },
    {
     "ename": "InvalidArgument",
     "evalue": "400 GPU Accelerator is required for image \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train:20231207_0936_RC00\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    945\u001b[0m                                       wait_for_ready, compression)\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"GPU Accelerator is required for image \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train:20231207_0936_RC00\".\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:74.125.69.95:443 {grpc_message:\"GPU Accelerator is required for image \\\"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train:20231207_0936_RC00\\\".\", grpc_status:3, created_time:\"2024-02-04T12:48:44.668803751+00:00\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-ff9132292101>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# Pass training arguments and launch job.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mmax_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m train_job.run(\n\u001b[0m\u001b[1;32m     51\u001b[0m     args=[\n\u001b[1;32m     52\u001b[0m         \u001b[0;34m\"--task=instruct-lora\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/aiplatform/training_jobs.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, dataset, annotation_schema_uri, model_display_name, model_labels, model_id, parent_model, is_default_version, model_version_aliases, model_version_description, base_output_dir, service_account, network, bigquery_destination, args, environment_variables, replica_count, machine_type, accelerator_type, accelerator_count, boot_disk_type, boot_disk_size_gb, reduction_server_replica_count, reduction_server_machine_type, reduction_server_container_uri, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, timeout, restart_job_on_worker_restart, enable_web_access, enable_dashboard_access, tensorboard, sync, create_request_timeout, disable_retries)\u001b[0m\n\u001b[1;32m   4628\u001b[0m         )\n\u001b[1;32m   4629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4630\u001b[0;31m         return self._run(\n\u001b[0m\u001b[1;32m   4631\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4632\u001b[0m             \u001b[0mannotation_schema_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mannotation_schema_uri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    815\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                     \u001b[0mVertexAiResourceNounWithFutureManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/aiplatform/training_jobs.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, dataset, annotation_schema_uri, worker_pool_specs, managed_model, model_id, parent_model, is_default_version, model_version_aliases, model_version_description, args, environment_variables, base_output_dir, service_account, network, bigquery_destination, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, timeout, restart_job_on_worker_restart, enable_web_access, enable_dashboard_access, tensorboard, reduction_server_container_uri, sync, create_request_timeout, block, disable_retries)\u001b[0m\n\u001b[1;32m   5334\u001b[0m         )\n\u001b[1;32m   5335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5336\u001b[0;31m         model = self._run_job(\n\u001b[0m\u001b[1;32m   5337\u001b[0m             \u001b[0mtraining_task_definition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefinition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_task\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5338\u001b[0m             \u001b[0mtraining_task_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_task_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/aiplatform/training_jobs.py\u001b[0m in \u001b[0;36m_run_job\u001b[0;34m(self, training_task_definition, training_task_inputs, dataset, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, annotation_schema_uri, model, model_id, parent_model, is_default_version, model_version_aliases, model_version_description, gcs_destination_uri_prefix, bigquery_destination, create_request_timeout, block)\u001b[0m\n\u001b[1;32m    816\u001b[0m         )\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         training_pipeline = self.api_client.create_training_pipeline(\n\u001b[0m\u001b[1;32m    819\u001b[0m             parent=initializer.global_config.common_location_path(\n\u001b[1;32m    820\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/aiplatform_v1/services/pipeline_service/client.py\u001b[0m in \u001b[0;36mcreate_training_pipeline\u001b[0;34m(self, request, parent, training_pipeline, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    761\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: 400 GPU Accelerator is required for image \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train:20231207_0936_RC00\"."
     ]
    }
   ],
   "source": [
    "# Huggingface dataset name or gs:// URI to a custom JSONL dataset.\n",
    "# dataset_name = \"timdettmers/openassistant-guanaco\"  # @param {type:\"string\"}\n",
    "dataset_name = \"<your cloud storage bucket url>/train.jsonl\"\n",
    "# Optional. Template name or gs:// URI to a custom template.\n",
    "template = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# Worker pool spec.\n",
    "# Find Vertex AI supported accelerators and regions in:\n",
    "#  https://cloud.google.com/vertex-ai/docs/training/configure-compute\n",
    "\n",
    "# # Uses V100 (16G) to finetune falcon-7b-instruct.\n",
    "# machine_type = \"n1-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "# # accelerator_type = \"NVIDIA_TESLA_P100\"\n",
    "# accelerator_count = 1\n",
    "\n",
    "# # # Uses L4 (24G) to finetune falcon-7b-instruct.\n",
    "# machine_type = \"g2-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 1\n",
    "\n",
    "# Uses V100 (16G) to finetune falcon-40b-instruct.\n",
    "# machine_type = \"n1-standard-16\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "# accelerator_count = 4\n",
    "\n",
    "# Uses L4 (24G) to finetune falcon-40b-instruct.\n",
    "# machine_type = \"g2-standard-24\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 2\n",
    "\n",
    "# Uses A100 (40G) to finetune falcon-40b-instruct.\n",
    "# machine_type = \"a2-highgpu-1g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "# accelerator_count = 1\n",
    "\n",
    "replica_count = 1\n",
    "\n",
    "# Setup training job.\n",
    "job_name = get_job_name_with_datetime(\"falcon-instruct-lora-train\")\n",
    "train_job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=job_name,\n",
    "    container_uri=TRAIN_DOCKER_URI,\n",
    ")\n",
    "output_dir = os.path.join(MODEL_BUCKET, job_name)\n",
    "output_dir_gcsfuse = output_dir.replace(\"gs://\", \"/gcs/\")\n",
    "\n",
    "# Pass training arguments and launch job.\n",
    "max_steps = 10\n",
    "train_job.run(\n",
    "    args=[\n",
    "        \"--task=instruct-lora\",\n",
    "        f\"--pretrained_model_id={base_model_id}\",\n",
    "        f\"--dataset_name={dataset_name}\",\n",
    "        f\"--output_dir={output_dir_gcsfuse}\",\n",
    "        \"--lora_rank=64\",\n",
    "        \"--lora_alpha=16\",\n",
    "        \"--lora_dropout=0.1\",\n",
    "        \"--warmup_ratio=0.03\",\n",
    "        f\"--max_steps={max_steps}\",\n",
    "        \"--max_seq_length=512\",\n",
    "        \"--learning_rate=2e-4\",\n",
    "        f\"--template={template}\",\n",
    "    ],\n",
    "    replica_count=replica_count,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    boot_disk_size_gb=500,\n",
    ")\n",
    "\n",
    "print(\"Trained models were saved in: \", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqmCtkGnhDmp"
   },
   "source": [
    "### Deploy\n",
    "This section uploads the model to Model Registry and deploys it on the Endpoint.\n",
    "\n",
    "The model deployment step will take 15 minutes to 40 minutes to complete.\n",
    "\n",
    "The peak GPU memory usages for [tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct), and [tiiuae/falcon-40b-instruct](https://huggingface.co/tiiuae/falcon-40b-instruct) with LoRA weights are ~15.5G and ~84G separately with the default settings. Please adjust the machine type, accelerator type and accelerator count accordingly. We use V100 in deployments as an example. Note that V100 serving generally offers better throughput and latency performance than L4 serving, while L4 serving is generally more cost efficient than V100 serving. The serving efficiency of V100 and L4 GPUs is inferior to that of A100 GPUs, but V100 and L4 GPUs are nevertheless good serving solutions if you do not have A100 quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "executionInfo": {
     "elapsed": 48138,
     "status": "error",
     "timestamp": 1707051153410,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "bf55e38815dc",
    "outputId": "2adcca62-b57e-489f-8de1-ae0f967e6e9e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/698487201632/locations/us-central1/endpoints/1215156061762224128/operations/1276185485454082048\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-7d4a08aac5e8>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# accelerator_count = 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m model_with_peft, endpoint_with_peft = deploy_model(\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_job_name_with_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"falcon-instruct-peft-serve\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mbase_model_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_model_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-2bacba521ac7>\u001b[0m in \u001b[0;36mdeploy_model\u001b[0;34m(model_name, base_model_id, finetuned_lora_model_path, service_account, task, machine_type, accelerator_type, accelerator_count)\u001b[0m\n\u001b[1;32m     25\u001b[0m ) -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n\u001b[1;32m     26\u001b[0m     \u001b[0;34m\"\"\"Deploys trained models into Vertex AI.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mendpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maiplatform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEndpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{model_name}-endpoint\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     serving_env = {\n\u001b[1;32m     29\u001b[0m         \u001b[0;34m\"BASE_MODEL_ID\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbase_model_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, display_name, description, labels, metadata, project, location, credentials, encryption_spec_key_name, sync, create_request_timeout, endpoint_id, enable_request_response_logging, request_response_logging_sampling_rate, request_response_logging_bq_destination_table)\u001b[0m\n\u001b[1;32m    423\u001b[0m                 )\n\u001b[1;32m    424\u001b[0m             )\n\u001b[0;32m--> 425\u001b[0;31m         return cls._create(\n\u001b[0m\u001b[1;32m    426\u001b[0m             \u001b[0mapi_client\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mdisplay_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplay_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    815\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                     \u001b[0mVertexAiResourceNounWithFutureManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(cls, api_client, display_name, project, location, description, labels, metadata, credentials, encryption_spec, network, sync, create_request_timeout, endpoint_id, predict_request_response_logging_config)\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_create_with_lro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_future\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0mcreated_endpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperation_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_create_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreated_endpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"endpoint\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \"\"\"\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocking_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36m_blocking_poll\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mpolling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_done_or_raise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRetryError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             raise concurrent.futures.TimeoutError(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             )\n\u001b[0;32m--> 349\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    350\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;34m\"Retrying due to {}, sleeping {:.1f}s ...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_exc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msleep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         )\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sleep generator stopped yielding sleep values.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Find Vertex AI supported accelerators and regions in:\n",
    "#  https://cloud.google.com/vertex-ai/docs/predictions/configure-compute\n",
    "\n",
    "# Sets V100 (16G) to deploy tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
    "# If A100 is not available, you may deploy tiiuae/falcon-40b-instruct with\n",
    "#  multiple V100s. Please keep in mind that the efficiency of serving with\n",
    "#  multiple V100s is inferior to that of serving with A100s.\n",
    "# Compared with L4, V100 serving can have better throughput and latency.\n",
    "machine_type = \"n1-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "accelerator_type = \"NVIDIA_TESLA_P100\"\n",
    "accelerator_count = 1  # for tiiuae/falcon-7b-instruct\n",
    "# accelerator_count = 8  # for tiiuae/falcon-40b-instruct\n",
    "\n",
    "# Sets L4 (24G) to deploy tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
    "# If A100 is not available, you may deploy tiiuae/falcon-40b-instruct with\n",
    "#  multiple L4s. Please keep in mind that the efficiency of serving with\n",
    "#  multiple L4s is inferior to that of serving with A100s.\n",
    "# Compared with V100, L4 serving can be more cost efficient.\n",
    "\n",
    "# For tiiuae/falcon-7b-instruct.\n",
    "# machine_type = \"g2-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 1\n",
    "\n",
    "# For tiiuae/falcon-40b-instruct.\n",
    "# machine_type = \"g2-standard-48\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 4\n",
    "\n",
    "# Sets A100 (40G) to deploy tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
    "# machine_type = \"a2-highgpu-1g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "# accelerator_count = 1  # for tiiuae/falcon-7b-instruct\n",
    "# accelerator_count = 4  # for tiiuae/falcon-40b-instruct\n",
    "\n",
    "# Sets A100 (80G) to deploy falcon-40b-instruct models for faster inferences.\n",
    "# machine_type = \"a2-ultragpu-1g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100_80GB\"\n",
    "# accelerator_count = 2\n",
    "\n",
    "model_with_peft, endpoint_with_peft = deploy_model(\n",
    "    model_name=get_job_name_with_datetime(prefix=\"falcon-instruct-peft-serve\"),\n",
    "    base_model_id=base_model_id,\n",
    "    finetuned_lora_model_path=os.path.join(output_dir, \"checkpoint-\" + str(max_steps)),\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    task=\"instruct-lora\",\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    ")\n",
    "print(\"endpoint_name:\", endpoint_with_peft.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80b3fd2ace09"
   },
   "source": [
    "NOTE: After the deployment succeeds, the base model weights will be downloaded one the fly from the original location and LoRA model weights will be downloaded from the GCS bucket used in training above. Thus, an additional 10-30 minutes of waiting time is needed **after** the above model deployment step succeeds and before you can run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint.\n",
    "\n",
    "Once deployment succeeds, you can send requests to the endpoint with text prompts.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Human: What is a car?\n",
    "Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ab04da3ec9a"
   },
   "outputs": [],
   "source": [
    "# Loads an existing endpoint instance using the endpoint name:\n",
    "# - Using `endpoint_name = endpoint_with_peft.name` allows us to get the\n",
    "#   endpoint name of the endpoint `endpoint_with_peft` created in the cell\n",
    "#   above.\n",
    "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
    "#   an existing endpoint with the ID 1234567890123456789.\n",
    "# You may uncomment the code below to load an existing endpoint.\n",
    "\n",
    "# endpoint_name = endpoint_with_peft.name\n",
    "# # endpoint_name = \"\"  # @param {type:\"string\"}\n",
    "# aip_endpoint_name = (\n",
    "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "# )\n",
    "# endpoint_with_peft = aiplatform.Endpoint(aip_endpoint_name)\n",
    "\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": \"What is a car?\",\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 10,\n",
    "    },\n",
    "]\n",
    "response = endpoint_with_peft.predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcbApnK4iyyu"
   },
   "source": [
    "## Quantize and deploy Falcon Instruct models\n",
    "\n",
    "This section demonstrates post-training quantization of Falcon Instruct models with Vertex Custom Job. Quantization reduces the memory required by a model while attempting to retain the same performance. Read more about GPTQ in the following publication: [GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rt9pFQV-jG1v"
   },
   "source": [
    "### Deploy pre-quantized models with Google Cloud Text Moderation\n",
    "Many GPTQ-quantized models are provided [here](https://huggingface.co/TheBloke?search_models=-gptq).\n",
    "\n",
    "This section uploads the model to Model Registry and deploys it on the Endpoint.\n",
    "\n",
    "The model deployment step will take 15 minutes to 1 hour to complete, depending on the model sizes.\n",
    "\n",
    "Notice that deploying a quantized requires much less GPU. We can deploy a quantized 40B model with only two L4s instead of four."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SFje2ifjNIx"
   },
   "outputs": [],
   "source": [
    "quantized_model_id = \"TheBloke/Falcon-7B-Instruct-GPTQ\"  # @param [\"TheBloke/Falcon-7B-Instruct-GPTQ\", \"TheBloke/falcon-40b-instruct-GPTQ\"]\n",
    "\n",
    "quantization_method = \"gptq\"\n",
    "\n",
    "# Finds Vertex AI prediction supported accelerators and regions in\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
    "\n",
    "# Sets 1 L4 (24G) to deploy Falcon Instruct 7B model.\n",
    "machine_type = \"g2-standard-8\"\n",
    "accelerator_type = \"NVIDIA_L4\"\n",
    "accelerator_count = 1\n",
    "\n",
    "# Sets 2 L4's (24G) to deploy Falcon Instruct 40B model.\n",
    "# machine_type = \"g2-standard-24\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 2\n",
    "\n",
    "model_prequantized_vllm, endpoint_prequantized_vllm = deploy_model_vllm(\n",
    "    model_name=get_job_name_with_datetime(\n",
    "        prefix=\"falcon-instruct-serve-vllm-prequantized\"\n",
    "    ),\n",
    "    model_id=quantized_model_id,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    quantization_method=quantization_method,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vpslc04szHW"
   },
   "source": [
    "NOTE: After the deployment succeeds, the model weights will be downloaded on the fly. Thus additional 10 ~ 40 minutes (depending on the model sizes) of waiting time is needed **after** the above model deployment step succeeds and before you run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint.\n",
    "\n",
    "Once deployment succeeds, you can send requests to the endpoint with text prompts.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Human: What is a car?\n",
    "Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPFMjyMEs0qt"
   },
   "outputs": [],
   "source": [
    "# Loads an existing endpoint instance using the endpoint name:\n",
    "# - Using `endpoint_name = endpoint_prequantized_vllm.name` allows us to get the\n",
    "#   endpoint name of the endpoint `endpoint_prequantized_vllm` created in the cell\n",
    "#   above.\n",
    "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
    "#   an existing endpoint with the ID 1234567890123456789.\n",
    "# You may uncomment the code below to load an existing endpoint.\n",
    "\n",
    "# endpoint_name = endpoint_prequantized_vllm.name\n",
    "# # endpoint_name = \"\"  # @param {type:\"string\"}\n",
    "# aip_endpoint_name = (\n",
    "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "# )\n",
    "# endpoint_prequantized_vllm = aiplatform.Endpoint(aip_endpoint_name)\n",
    "\n",
    "\n",
    "# Overides max_length and top_k parameters during inferences.\n",
    "# If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`,\n",
    "# you can reduce the max length, such as set max_length as 20.\n",
    "# If you are using L4 GPUs to serve Falcon Instruct 40B models, you should set\n",
    "# max_length to around 1,000 tokens or fewer. If you need longer generated\n",
    "# sequences, please file a request with Vertex to allowlist your project for a\n",
    "# longer timeout threshold with Vertex endpoints.\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": \"What is a car?\",\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 10,\n",
    "    },\n",
    "]\n",
    "response = endpoint_prequantized_vllm.predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0Xjw3Gxs_Lv"
   },
   "source": [
    "Text moderation analyzes a document against a list of safety attributes, which include \"harmful categories\" and topics that may be considered sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GR2a8rzvs_kk"
   },
   "outputs": [],
   "source": [
    "for generated_text in response.predictions:\n",
    "    # Send a request to the API.\n",
    "    response = moderate_text(generated_text)\n",
    "    # Show the results.\n",
    "    show_text_moderation(generated_text, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApFQMgVKmUbD"
   },
   "source": [
    "### Quantize Falcon Instruct models\n",
    "\n",
    "Quantization reduces the amount of GPU required to serve a model by reducing the bit precision of the weights while minimizing drop in performance. Serving quantized models on VLLM requires models to be quantized to 4 bits. It is recommended to first search if a model has already been quantized and made publicly available: [GPTQ](https://huggingface.co/TheBloke?search_models=-gptq).\n",
    "\n",
    "Quantizing models with GPTQ will take around 40 minutes for Falcon Instruct 7B using 1 NVIDIA_L4 GPU and will take around 4 hours for Falcon Instruct 40B using 4 NVIDIA_L4 GPU.\n",
    "\n",
    "Finetuned models can also be quantized, so long as the LoRA weights are merged with the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wz8V8oNkxdEZ"
   },
   "source": [
    "Set the base model id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oeyvnj8BxX8K"
   },
   "outputs": [],
   "source": [
    "base_model_id = \"tiiuae/falcon-7b-instruct\"  # @param [\"tiiuae/falcon-7b-instruct\", \"tiiuae/falcon-40b-instruct\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEquWSCkmrJ6"
   },
   "outputs": [],
   "source": [
    "# Set up quantization job.\n",
    "\n",
    "# Set `finetuned_model_path` to the GCS path of the merged finetuned model\n",
    "# from the section above, if not set, the base model will be quantized.\n",
    "finetuned_model_path = \"\"  # @param {type:\"string\"}\n",
    "if finetuned_model_path:\n",
    "    prequantized_model_path = finetuned_model_path\n",
    "else:\n",
    "    prequantized_model_path = base_model_id\n",
    "\n",
    "quantization_method = \"gptq\"\n",
    "quantization_job_name = get_job_name_with_datetime(\n",
    "    f\"falcon-instruct-{quantization_method}-quantize\"\n",
    ")\n",
    "\n",
    "quantization_output_dir = os.path.join(MODEL_BUCKET, quantization_job_name)\n",
    "quantization_output_dir_gcsfuse = quantization_output_dir.replace(\"gs://\", \"/gcs/\")\n",
    "\n",
    "# Worker pool spec.\n",
    "\n",
    "# Set 1 L4 (24G) for quantizing 7b model.\n",
    "machine_type = \"g2-standard-32\"\n",
    "accelerator_type = \"NVIDIA_L4\"\n",
    "accelerator_count = 1\n",
    "\n",
    "# Set 4 L4 (24G) for quantizing 40b model.\n",
    "# machine_type = \"g2-standard-48\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 4\n",
    "\n",
    "# Quantization parameters.\n",
    "quantization_precision_mode = \"4bit\"\n",
    "\n",
    "# The original datasets used in GPTQ paper.\n",
    "gptq_dataset_name = \"c4\"  # @param [\"wikitext2\",\"c4\",\"c4-new\",\"ptb\",\"ptb-new\"]\n",
    "group_size = -1\n",
    "damp_percent = 0.1\n",
    "desc_act = True\n",
    "quantization_args = [\n",
    "    \"--task=quantize-model\",\n",
    "    f\"--quantization_method={quantization_method}\",\n",
    "    f\"--pretrained_model_id={base_model_id}\",\n",
    "    f\"--quantization_precision_mode={quantization_precision_mode}\",\n",
    "    f\"--quantization_output_dir={quantization_output_dir_gcsfuse}\",\n",
    "    f\"--quantization_dataset_name={gptq_dataset_name}\",\n",
    "    f\"--group_size={group_size}\",\n",
    "    f\"--damp_percent={damp_percent}\",\n",
    "    f\"--desc_act={desc_act}\",\n",
    "    \"--cache_examples_on_gpu=False\",\n",
    "]\n",
    "\n",
    "# Pass quantization arguments and launch job.\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": machine_type,\n",
    "            \"accelerator_type\": accelerator_type,\n",
    "            \"accelerator_count\": accelerator_count,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"disk_spec\": {\n",
    "            \"boot_disk_type\": \"pd-ssd\",\n",
    "            \"boot_disk_size_gb\": 500,\n",
    "        },\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_DOCKER_URI,\n",
    "            \"env\": [\n",
    "                {\n",
    "                    \"name\": \"PYTORCH_CUDA_ALLOC_CONF\",\n",
    "                    \"value\": \"max_split_size_mb:32\",\n",
    "                },\n",
    "            ],\n",
    "            \"command\": [],\n",
    "            \"args\": quantization_args,\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Quantizing {prequantized_model_path}.\")\n",
    "quantize_job = aiplatform.CustomJob(\n",
    "    display_name=quantization_job_name,\n",
    "    project=PROJECT_ID,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    ")\n",
    "quantize_job.run()\n",
    "\n",
    "print(\"Quantized models were saved in: \", quantization_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7GIC4AYrG0E"
   },
   "source": [
    "### Deploy quantized models with Google Cloud Text Moderation\n",
    "This section uploads the model to Model Registry and deploys it on the Endpoint.\n",
    "\n",
    "The model deployment step will take 15 minutes to 1 hour to complete, depending on the model sizes.\n",
    "\n",
    "Notice that deploying a quantized model requires much less GPU. We can deploy a quantized 40B model with only two L4 GPUs instead of four."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SpJ_lbatIf7"
   },
   "outputs": [],
   "source": [
    "# Finds Vertex AI prediction supported accelerators and regions in\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
    "\n",
    "# Sets 1 L4 (24G) to deploy Falcon Instruct 7B.\n",
    "machine_type = \"g2-standard-8\"\n",
    "accelerator_type = \"NVIDIA_L4\"\n",
    "accelerator_count = 1\n",
    "\n",
    "# Sets 2 L4's (24G) to deploy Falcon Instruct 70B models.\n",
    "# machine_type = \"g2-standard-24\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 2\n",
    "\n",
    "model_quantized_vllm, endpoint_quantized_vllm = deploy_model_vllm(\n",
    "    model_name=get_job_name_with_datetime(\n",
    "        prefix=\"falcon-instruct-serve-vllm-quantized\"\n",
    "    ),\n",
    "    model_id=quantization_output_dir,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    quantization_method=quantization_method,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dzq7aM6ctPD9"
   },
   "source": [
    "NOTE: After the deployment succeeds, the model weights will be downloaded on the fly. Thus additional 10 ~ 40 minutes (depending on the model sizes) of waiting time is needed **after** the above model deployment step succeeds and before you run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint.\n",
    "\n",
    "Once deployment succeeds, you can send requests to the endpoint with text prompts.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Human: What is a car?\n",
    "Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twm_awnMtQa2"
   },
   "outputs": [],
   "source": [
    "# Loads an existing endpoint instance using the endpoint name:\n",
    "# - Using `endpoint_name = endpoint_quantized_vllm.name` allows us to get the\n",
    "#   endpoint name of the endpoint `endpoint_quantized_vllm` created in the cell\n",
    "#   above.\n",
    "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
    "#   an existing endpoint with the ID 1234567890123456789.\n",
    "# You may uncomment the code below to load an existing endpoint.\n",
    "\n",
    "# endpoint_name = endpoint_quantized_vllm.name\n",
    "# # endpoint_name = \"\"  # @param {type:\"string\"}\n",
    "# aip_endpoint_name = (\n",
    "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "# )\n",
    "# endpoint_quantized_vllm = aiplatform.Endpoint(aip_endpoint_name)\n",
    "\n",
    "\n",
    "# Overides max_length and top_k parameters during inferences.\n",
    "# If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`,\n",
    "# you can reduce the max length, such as set max_length as 20.\n",
    "# If you are using L4 GPUs to serve Falcon Instruct 40B models, you should set\n",
    "# max_length to around 1,000 tokens or fewer. If you need longer generated\n",
    "# sequences, please file a request with Vertex to allowlist your project for a\n",
    "# longer timeout threshold with Vertex endpoints.\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": \"What is a car?\",\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 10,\n",
    "    },\n",
    "]\n",
    "response = endpoint_quantized_vllm.predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AeUE4K_CteAj"
   },
   "source": [
    "Text moderation analyzes a document against a list of safety attributes, which include \"harmful categories\" and topics that may be considered sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAoVzA69teui"
   },
   "outputs": [],
   "source": [
    "for generated_text in response.predictions:\n",
    "    # Send a request to the API.\n",
    "    response = moderate_text(generated_text)\n",
    "    # Show the results.\n",
    "    show_text_moderation(generated_text, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ffffac5b20a"
   },
   "source": [
    "## Evaluate PEFT-finetuned Falcon Instruct models\n",
    "\n",
    "This section demonstrates how to evaluate the Falcon Instruct models fintuned with PEFT LoRA using EleutherAI's [Language Model Evaluation Harness (lm-evaluation-harness)](https://github.com/EleutherAI/lm-evaluation-harness) with Vertex CustomJob. Please reference the peak GPU memory usgaes for serving and adjust the machine type, accelerator type and accelerator count accordingly.\n",
    "\n",
    "This example uses the dataset [TruthfulQA](https://arxiv.org/abs/2109.07958). All supported tasks are listed in [this task table](https://github.com/EleutherAI/lm-evaluation-harness/blob/master/docs/task_table.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "078b4d178624"
   },
   "outputs": [],
   "source": [
    "eval_dataset = \"truthfulqa_mc\"  # @param {type:\"string\"}\n",
    "\n",
    "# Worker pool spec.\n",
    "# Find Vertex AI supported accelerators and regions in:\n",
    "#  https://cloud.google.com/vertex-ai/docs/training/configure-compute\n",
    "\n",
    "# Sets V100 (16G) to evaluate tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
    "# If A100 is not available, you may evaluate tiiuae/falcon-40b-instruct with\n",
    "#  multiple V100s. Please keep in mind that the efficiency of evaluating with\n",
    "#  multiple V100s is inferior to that of evaluating with A100s.\n",
    "# Compared with L4, V100 inference can have better throughput and latency.\n",
    "machine_type = \"n1-standard-8\"\n",
    "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "accelerator_count = 1  # for tiiuae/falcon-7b-instruct\n",
    "# accelerator_count = 8  # for tiiuae/falcon-40b-instruct\n",
    "\n",
    "# Sets L4 (24G) to evaluate tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
    "# If A100 is not available, you may evaluate tiiuae/falcon-40b-instruct with\n",
    "#  multiple L4s. Please keep in mind that the efficiency of evaluating with\n",
    "#  multiple L4s is inferior to that of evaluating with A100s.\n",
    "# Compared with V100, L4 inference can be more cost efficient.\n",
    "\n",
    "# For tiiuae/falcon-7b-instruct.\n",
    "# machine_type = \"g2-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 1\n",
    "\n",
    "# For tiiuae/falcon-40b-instruct.\n",
    "# machine_type = \"g2-standard-48\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "# accelerator_count = 4\n",
    "\n",
    "# Sets A100 (40G) to evaluate tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
    "# machine_type = \"a2-highgpu-1g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "# accelerator_count = 1  # for tiiuae/falcon-7b-instruct\n",
    "# accelerator_count = 4  # for tiiuae/falcon-40b-instruct\n",
    "\n",
    "# Sets A100 (80G) to evaluate falcon-40b-instruct models for faster inferences.\n",
    "# machine_type = \"a2-ultragpu-1g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100_80GB\"\n",
    "# accelerator_count = 2\n",
    "\n",
    "replica_count = 1\n",
    "\n",
    "# Setup evaluation job.\n",
    "job_name = get_job_name_with_datetime(prefix=\"falcon-instruct-peft-eval\")\n",
    "eval_output_dir = os.path.join(MODEL_BUCKET, job_name)\n",
    "eval_output_dir_gcsfuse = eval_output_dir.replace(\"gs://\", \"/gcs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0t0RBixIw0P"
   },
   "outputs": [],
   "source": [
    "# Prepare evaluation command that runs the evaluation harness.\n",
    "# Set `trust_remote_code = True` because evaluating the model requires\n",
    "# executing code from the model repository.\n",
    "# Set `use_accelerate = True` to enable evaluation across multiple GPUs.\n",
    "eval_command = [\n",
    "    \"python\",\n",
    "    \"main.py\",\n",
    "    \"--model\",\n",
    "    \"hf-causal-experimental\",\n",
    "    \"--model_args\",\n",
    "    f\"pretrained={base_model_id},peft={output_dir_gcsfuse},trust_remote_code=True,use_accelerate=True,device_map_option=auto\",\n",
    "    \"--tasks\",\n",
    "    f\"{eval_dataset}\",\n",
    "    \"--output_path\",\n",
    "    f\"{eval_output_dir_gcsfuse}\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afd9305771ba"
   },
   "source": [
    "### Submit evaluation CustomJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "602413896b58"
   },
   "outputs": [],
   "source": [
    "# Pass evaluation arguments and launch job.\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": machine_type,\n",
    "            \"accelerator_type\": accelerator_type,\n",
    "            \"accelerator_count\": accelerator_count,\n",
    "        },\n",
    "        \"replica_count\": replica_count,\n",
    "        \"disk_spec\": {\n",
    "            \"boot_disk_size_gb\": 500,\n",
    "        },\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": EVAL_DOCKER_URI,\n",
    "            \"command\": eval_command,\n",
    "            \"args\": [],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "eval_job = aiplatform.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    base_output_dir=eval_output_dir,\n",
    ")\n",
    "\n",
    "eval_job.run()\n",
    "\n",
    "print(\"Evaluation results were saved in:\", eval_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de47e182a37e"
   },
   "source": [
    "### Fetch and print evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1f15ed6d375a"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "# Fetch evaluation results.\n",
    "storage_client = storage.Client()\n",
    "BUCKET_NAME = BUCKET_URI.split(\"gs://\")[1]\n",
    "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "RESULT_FILE_PATH = eval_output_dir[len(BUCKET_URI) + 1 :]\n",
    "blob = bucket.blob(RESULT_FILE_PATH)\n",
    "raw_result = blob.download_as_string()\n",
    "\n",
    "# Print evaluation results.\n",
    "result = json.loads(raw_result)\n",
    "result_formatted = json.dumps(result, indent=2)\n",
    "print(f\"Evaluation result:\\n{result_formatted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af21a3cff1e0"
   },
   "source": [
    "## Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "911406c1561e"
   },
   "outputs": [],
   "source": [
    "# Delete custom train, quantization, and evaluation jobs.\n",
    "train_job.delete()\n",
    "quantize_job.delete()\n",
    "eval_job.delete()\n",
    "\n",
    "# Undeploy models and delete endpoints.\n",
    "endpoint_without_peft.delete(force=True)\n",
    "endpoint_with_peft.delete(force=True)\n",
    "endpoint_prequantized_vllm.delete(force=True)\n",
    "endpoint_quantized_vllm.delete(force=True)\n",
    "\n",
    "# Delete models.\n",
    "model_without_peft.delete()\n",
    "model_with_peft.delete()\n",
    "model_prequantized_vllm.delete()\n",
    "model_quantized_vllm.delete()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_garden_pytorch_falcon_instruct_peft.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "177a2ca248a04f018f2b33c62a0b69ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_84823bb80b4e46059ec9475365eb50f7",
       "IPY_MODEL_a7d695824ab24305886e9a99e9c18ee4",
       "IPY_MODEL_47c36d1369ff4bf69cf2f08d417ec44d"
      ],
      "layout": "IPY_MODEL_43bb13013339485ea1db7307f574f0dd"
     }
    },
    "1c0b3e3c081745b6b468f320c9fb1e92": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43bb13013339485ea1db7307f574f0dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "47c36d1369ff4bf69cf2f08d417ec44d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c0b3e3c081745b6b468f320c9fb1e92",
      "placeholder": "​",
      "style": "IPY_MODEL_b00f3bad6d704cf8a91b1c49053299d4",
      "value": " 0/2 [00:00&lt;?, ?it/s]"
     }
    },
    "4dc1bce972934f639a73d749267d2847": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6aa23af6554f41cba9744250bd0eabc2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "84823bb80b4e46059ec9475365eb50f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6aa23af6554f41cba9744250bd0eabc2",
      "placeholder": "​",
      "style": "IPY_MODEL_e5c32638f4244f1fb3c005e48a10911e",
      "value": "Loading checkpoint shards:   0%"
     }
    },
    "9396e90f6c2247d8bfb018617241ef43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a7d695824ab24305886e9a99e9c18ee4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4dc1bce972934f639a73d749267d2847",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9396e90f6c2247d8bfb018617241ef43",
      "value": 0
     }
    },
    "b00f3bad6d704cf8a91b1c49053299d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e5c32638f4244f1fb3c005e48a10911e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
